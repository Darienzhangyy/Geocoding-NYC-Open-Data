---
title: "hw 3"
author: "Eclectic Eagle Scouts"
date: "10/26/2015"
output: pdf_document
---

a description of any and all cleaning / subsetting / etc. that was done to the data, as well as a description of your geocoding approach(es) and a discussion of how successful each was:

To clean the data in nyc.311csv as much as possible while keeping address information as complete as possible, we apply a combination of methods (subsetting, removing NA values, etc.) to `nyc`. We then apply gecoding approaches to the `intersection`, and `pluto.Rdata` and finally we merge data from these cleaned datasets and produce a graph of New York City. 

To clean up the data, we extract columns from nyc.311csv file that contains `Unique.Key`, any column that contains `Address` and `Borough` and clean up those address columns as much as possible. We implement the following procedures: 

(1) remove `nyc` rows with neither $ZIP code nor $Borough; (2) remove `nyc` rows with non-empty $ZIP code values of less than 5 characters AND no assigned $Borough; (3) Extract the first 5 characters of all 9- and 10-character $ZIP code values, which are likely ZIP+4 values; (4) Assign NAs to all $ZIP code values not containing exactly 5 digits. Number of observations dropps from more than 10 million to 9.7 million after these four steps. (5) For all non-NA $ZIP code values with at least one $Borough value, determine the most common such $Borough value, and reassign the $Borough value of corresponding entries in `nyc` accordingly; (6) Remove `nyc` rows WITHOUT informative $Borough, $ZIP, or $Incident.Address values and rows which contain NAs for $Borough, $ZIP, AND $Incident.Address; (7) Subset `nyc`, keeping rows with EITHER a street and cross street, two intersection streets, or an incident address; (8) clean up other files and only retain the 'nyc' file. 

To geocode the data, we use readOGR to read in `intersections` shapefile and extract the coordinates and attached data into `data` and implement the following procedures: 

(1) Subset `data`, keeping rows with ONLY two non-boundary listings as streets; (2) Split these intersections into separate columns; (3) Omit duplicate rows and unneeded columns; (4) Rename columns and sort by first and second streets; (5) cleaned up the streets names by implementing a function named standardize_streets() that takes input `col`, a column in `data` data frame and produces an output `the same column`, but with standardized address formatting; (5) CLean up the environment but keep file `nyc` and `data`; 
(6) Merge `nyc` and `data`, matching `$Street.Name` to `$street_1` or `$street_2` and `$Cross.Street.1` to `$street_2` or `$street_1`;
(7)Clean up the files but keep `nyc`, `data` and `h1`from the merge; (8) Merge `nyc` and `data`, matching `$Intersection.Street.1` to `$street_1` or `$street_2` and `$Intersection.Street.2` to `$street_2` or `$street_1`;
(9)Clean up the environment but keep `h1` and `h2`; (10)Merge `nyc` and `data` into data frame called `geocoded` and write to disk;
(11)Remove duplicated addresses and clean up the environment but keep `geocoded_unique`.

To merge data, we then load the pluto.Rdata and join the pluto data with `geocoded_unique` from above and clean up the environment but keep file `full`.Then we implement the following steps: 

(1)Calculate mean Pluto-sourced latitude and longitude coordinates for all rows sharing the same `$Incident.Address` and `$Borough.y` values; (2) Calculate mean Intersections-sourced latitude and longitude coordinates for all rows sharing the same `$Incident.Address` and `$Borough.y` values; (3)Merge `pluto_mean` and `inter_mean` into one data frame; (4)Plot locations with colors corresponding to boroughs.


**Part 2**

We used the dataframe generated from part 1 to produce JSON file but produced a high score. Therefore we modified our approach to just join the data of incident streets with data of pluto by matching "Address". As a result, we produced a dataframe named `clean_data` that contains longitude, latitude and matched addresses. Then we use the SVM model to fit those data and plot the polygons using the prediction function. The code is specified in 'graph2.R'. 

```{r}
source('graph2.R')
plot(poly)
```


**Part 3 visualization**

We are interested in visualizing how different number of complaints is across each borough in an aggregation level, as the complaints are one of the most informative and important parts in the dataset, and we can use the previous data obtained to pinpoint where a specifice complaint happened. Therefore, we draw a heatmap of total complaints in New York City using the library of ggmap and ggplot. We first use ggmap to load a map of New York City, then plot the heatmapuse using the paired longitude and latitude obtained from the previous data. The result shows that Brooklyn has the most complaints among the five boroughs. 

```{r}
library('ggplot2')
library('ggmap')

#Load complain data "with_null.RData"
load("with_null.RData")

## Get New YorK City map
NYC <- get_map("New York City", zoom = 11)
ggmap(NYC)

## Get crime locations
full_data$Longitude <- round(as.numeric(full_data$Longitude), 2)
full_data$Latitude <- round(as.numeric(full_data$Latitude), 2)

#create a data frame containing the coordinates of all complains
locationCrimes <- as.data.frame(table(full_data$Longitude, full_data$Latitude))

names(locationCrimes) <- c('long', 'lat', 'Frequency')
locationCrimes$long <- as.numeric(as.character(locationCrimes$long))
locationCrimes$lat <- as.numeric(as.character(locationCrimes$lat))

## Plotting the location heatmap
ggmap(NYC) + geom_tile(data = locationCrimes, aes(x = long, y = lat, alpha = Frequency),
                           fill = 'red') + theme(axis.title.y = element_blank(), axis.title.x = element_blank())

```


